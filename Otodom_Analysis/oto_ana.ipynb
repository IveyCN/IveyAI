{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9131a22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.point import Point\n",
    "from snowflake.sqlalchemy import URL\n",
    "from sqlalchemy import create_engine\n",
    "from snowflake.connector.pandas_tools import pd_writer\n",
    "import time \n",
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fc6eab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94c8f5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "geolocator = Nominatim(user_agent=\"otodomprojectanalysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b37cb7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = create_engine(URL(\n",
    "                    account = 'ZZMZZDH-AR84667',\n",
    "                    user = 'yifanj4',\n",
    "                    password = 'Z970710z',\n",
    "                    database = 'demo',\n",
    "                    schema = 'public',\n",
    "                    warehouse = 'demo_wh'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28685d5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'engine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m engine\u001b[38;5;241m.\u001b[39mconnect() \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      3\u001b[0m         query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m SELECT RN, concat(latitude,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,longitude) as LOCATION\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124m                    FROM (SELECT RN\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124m                            , SUBSTR(location, REGEXP_INSTR(location,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,1,4)+1) AS LATITUDE \u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124m                            , SUBSTR(location, REGEXP_INSTR(location,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,1,1)+1, (REGEXP_INSTR(location,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,1,2) - REGEXP_INSTR(location,\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,1,1) - 1) ) AS LONGITUDE\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124m                        FROM otodom_data_flatten\u001b[39m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124m                        ORDER BY rn  ) \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'engine' is not defined"
     ]
    }
   ],
   "source": [
    "with engine.connect() as conn:\n",
    "    try:\n",
    "        query = \"\"\" SELECT RN, concat(latitude,',',longitude) as LOCATION\n",
    "                    FROM (SELECT RN\n",
    "                            , SUBSTR(location, REGEXP_INSTR(location,' ',1,4)+1) AS LATITUDE \n",
    "                            , SUBSTR(location, REGEXP_INSTR(location,' ',1,1)+1, (REGEXP_INSTR(location,' ',1,2) - REGEXP_INSTR(location,' ',1,1) - 1) ) AS LONGITUDE\n",
    "                        FROM otodom_data_flatten\n",
    "                        ORDER BY rn  ) \"\"\"\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "        \n",
    "        df = pd.read_sql(query,conn)\n",
    "                      \n",
    "        df.columns = map(lambda x: str(x).upper(), df.columns)\n",
    "        \n",
    "        ddf = dd.from_pandas(df,npartitions=10)\n",
    "        print(ddf.head(5,npartitions=-1))\n",
    "\n",
    "        ddf['ADDRESS'] = ddf['LOCATION'].apply(lambda x: geolocator.reverse(x).raw['address'],meta=(None, 'str'))\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "        pandas_df = ddf.compute()\n",
    "        print(pandas_df.head())\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "        pandas_df.to_sql('otodom_data_flatten_address', con=engine, if_exists='append', index=False, chunksize=16000, method=pd_writer)\n",
    "    except Exception as e:\n",
    "        print('--- Error --- ',e)\n",
    "    finally:\n",
    "        conn.close()\n",
    "engine.dispose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e24843c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gspread\n",
    "from gspread_dataframe import get_as_dataframe, set_with_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e3ac907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Error ---  {'code': 403, 'message': 'Rate limit exceeded. User message: \"Sorry, you have exceeded your sharing quota.\"', 'errors': [{'message': 'Rate limit exceeded. User message: \"Sorry, you have exceeded your sharing quota.\"', 'domain': 'usageLimits', 'reason': 'sharingRateLimitExceeded'}]}\n",
      "--- 395.92563819885254 seconds ---\n"
     ]
    }
   ],
   "source": [
    "with engine.connect() as conn:\n",
    "    try:\n",
    "        query = \"\"\" SELECT RN, TITLE FROM otodom_data_flatten ORDER BY rn\"\"\"\n",
    "\n",
    "        df = pd.read_sql(query,conn)\n",
    "\n",
    "        gc = gspread.service_account()\n",
    "\n",
    "        loop_counter = 0\n",
    "        chunk_size = 1000\n",
    "        file_name = 'OTODOM_ANALYSIS_'\n",
    "        user_email = 'yjian0235@gmail.com'\n",
    "\n",
    "        for i in range(0,len(df),chunk_size):\n",
    "            loop_counter += 1\n",
    "            df_in = df.iloc[i:(i+chunk_size), :]\n",
    "\n",
    "            spreadsheet_title = file_name + str(loop_counter)\n",
    "            try:\n",
    "                locals()['sh'+str(loop_counter)] = gc.open(spreadsheet_title)\n",
    "            except gspread.SpreadsheetNotFound:\n",
    "                locals()['sh'+str(loop_counter)] = gc.create(spreadsheet_title)\n",
    "\n",
    "            locals()['sh'+str(loop_counter)].share(user_email, perm_type='user', role='writer')\n",
    "            wks = locals()['sh'+str(loop_counter)].get_worksheet(0)\n",
    "            wks.resize(len(df_in)+1)\n",
    "            set_with_dataframe(wks, df_in)   \n",
    "                \n",
    "            column = 'C'   # Column to apply the formula \n",
    "            start_row = 2  # Starting row to apply the formula\n",
    "            end_row = wks.row_count   # Ending row to apply the formula\n",
    "            cell_range = f'{column}{start_row}:{column}{end_row}' \n",
    "            curr_row = start_row\n",
    "            cell_list = wks.range(cell_range)\n",
    "            \n",
    "            for cell in cell_list:\n",
    "                cell.value = f'=GOOGLETRANSLATE(B{curr_row},\"pl\",\"en\")'\n",
    "                curr_row += 1\n",
    "                \n",
    "            # Update the worksheet with the modified cells\n",
    "            wks.update_cells(cell_list, value_input_option='USER_ENTERED')\n",
    "\n",
    "            print(f'Spreadsheet {spreadsheet_title} created!')\n",
    "\n",
    "            df_log = pd.DataFrame({'ID':[loop_counter], 'SPREADSHEET_NAME':[spreadsheet_title]})\n",
    "            df_log.to_sql('otodom_data_log', con=engine, if_exists='append', index=False, chunksize=16000, method=pd_writer)\n",
    "\n",
    "\n",
    "            # df_out = get_as_dataframe(wks, usecols=[0,1,2,3], nrows=end_row, header=None, skiprows=1)\n",
    "            # print(f'Spreadsheet {locals()[\"sh\"+str(loop_counter)]} loaded back to DataFrame!')\n",
    "            \n",
    "            # df_out.columns = ['RN', 'TITLE', 'LOCATION', 'TITLE_ENG']\n",
    "\n",
    "            # df_out.to_sql('otodom_data_transformed', con=engine, if_exists='append', index=False, chunksize=16000, method=pd_writer)\n",
    "\n",
    "    except Exception as e:\n",
    "        print('--- Error --- ',e)\n",
    "    finally:\n",
    "        conn.close()\n",
    "engine.dispose()\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01050d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.connect() as conn:\n",
    "    try:\n",
    "        query = \"\"\" SELECT ID, SPREADSHEET_NAME FROM otodom_data_log \"\"\"\n",
    "        df = pd.read_sql(query,conn)\n",
    "        df.columns = map(lambda x: str(x).upper(), df.columns)\n",
    "\n",
    "        gc = gspread.service_account()\n",
    "        loop_counter = 0\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            loop_counter += 1\n",
    "            locals()['sh'+str(loop_counter)] = gc.open(row['SPREADSHEET_NAME'])\n",
    "            wks = locals()['sh'+str(loop_counter)].get_worksheet(0)\n",
    "            df_out = get_as_dataframe(wks, usecols=[0,1,2], nrows=wks.row_count, header=None, skiprows=1, evaluate_formulas=True)\n",
    "            print('Spreadsheet '+row['SPREADSHEET_NAME']+' loaded back to DataFrame!')\n",
    "            \n",
    "            df_out.columns = ['RN', 'TITLE', 'TITLE_ENG']\n",
    "            df_out.to_sql('otodom_data_flatten_translate', con=engine, if_exists='append', index=False, chunksize=16000, method=pd_writer)\n",
    "\n",
    "    except Exception as e:\n",
    "        print('--- Error --- ',e)\n",
    "    finally:\n",
    "        conn.close()\n",
    "engine.dispose()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
